[
["index.html", "Intro To Research Computing Chapter 1 Computation In Research 1.1 What Do the Top Sites Use ? 1.2 Division of Labor 1.3 Popular Data Science Languages 1.4 Is Windows Good For Computation ? 1.5 What’s Wrong with Windows ? 1.6 Is There a Middle Ground ? 1.7 Apple OSX / macOS ?", " Intro To Research Computing Steve Pittard 2019-04-16 Chapter 1 Computation In Research Biomedical and Public Health Research is increasingly reliant upon computation to achieve its aims. There are a number of considerations to make when thinking about how best to conduct computationally-assisted work. This also includes ideas on how to develop, maintain, and implement software pipelines. When doing literature reviews it is increasingly common to see references to programs written in R, Python, Java as associations with databases, REST APIs, and various cloud architectures. What this means is that it is incumbent upon researchers to learn something about analytics tools and supporting development languages to be competitive. Hopefully, this information can clear up some confusion and provide some basic ideas on how to be productive. Summary: Linux and UNIX (considered here as one in the same although technically that’s not true) has the largest market share by far when considering computational research. If you intend on pursing research and want to use computation at scale then Linux is the way to go. You can still use OSX or Windows as a way to access Linux servers or you could use a Linux desktop. We’ll learn about accessing remote LINUX servers and cloud instances. It’s always helpful to understand the trends to see where things are going. At one time most of the world’s computing took place on privately owned resources. It turns out that that is still the case. Check out the following: 1.1 What Do the Top Sites Use ? If you are fortunate then you have access to a professionally maintained computational cluster that has all the packages and languages you need along with a highly responsive staff who will answer questions for you and provide assistance in the design of large scale runs. Most people do not have access to such resources or if they do it is basic access to a cluster with little or no add on support. It depends on your department and institution. Consider the site Top 500 website whose mission is to provide information on the Top 500 most significant computational installations in the world. This is important not just from an enthusiast or geek point of view but as an indicator of what it takes to support aggressive types of computational research. Statistics on high-performance computers are of major interest to manufacturers, users, and potential users. These people wish to know not only the number of systems installed, but also the location of the various supercomputers within the high-performance computing community and the applications for which a computer system is being used. Such statistics can facilitate the establishment of collaborations, the exchange of data and software, and provide a better understanding of the high-performance computer market. 1.2 Division of Labor There is a natural tendency to use one’s laptop or desktop computer to do some analysis which, in a general sense, is okay. This can be very useful for learning a particular approach or becoming familiar with a package you read about in a research paper. The laptop can also be a great place to work on creating prototypical applications that are modest in size with the idea of eventually moving them to larger infrastructure for scaling. 1.2.1 Mixing Work and Pleasure When getting a desktop or laptop everyone starts out with the best of intentions by wanting to devote the computer to a specific set of tasks. Although over time the laptop becomes a mixture of work and pleasure along with some activities that involve both. If you organize your hard drive well then you can easily make distinctions between personal and work data but eventually it becomes unclear as to what is what. Also keep in mind that most laptops and desktops are configured and optimized to support general office productivity such as accessing the Internet, reading E-Mail, and preparing documents. One can also install applications such as SAS, SPSS, or R-Studio to do some analysis and if the size of the input data is manageable then this is fine. 1.2.2 Mixing E-Mail and Research ? The trouble starts when you have to install tools that require additional configuration or adjustments to system permissions which might then impact other programs on the system. Usually this isn’t the case but I’ve filled up my hard drive with data files on a system that is ostensibly for personal productivity. So not only does the system become unwieldy for analysis but then I have no room for my office stuff. Let’s put it another way, you do not want to be doing serious research and computation on the same computer you normally read e-mail on. I’ve seen people trying to open e-mail attachments that causes the system to reboot thus destroying a long running process. 1.2.3 You’ll Do It Anyway The lesson here is that once you hit a certain threshold of analytics activity or start managing large data sets that gobble up most of you hard drive then it’s reasonable to consider using a server or external computer that is optimized for analysis and data storage. At a minimum, you should move your data files over to a shared drive (if one is available) and refer to that location when doing work. Obviously, when going home you will need to make local copies or use the VPN to attach to the shared drive from home. 1.3 Popular Data Science Languages Let’s take a look at one survey that rates the most popular languages and frameworks used in Data Science. This is from the KDNuggets website. These aren’t the only languages and there are some languages like Julia that are gaining some market share. The way to think about lists like this is that if you need a lot of these then you are probably heading in a direction that would benefit from using a Linux based server or a virtual machine or Docker of some type. Especially if you have lots of data to clean and transform. But you could install most of these tools on a laptop running Windows or OSX. Also remember that you would also use things like SAS, SPSS, Systat, Minitab, etc. 1.4 Is Windows Good For Computation ? The Windows Operating System has a rich history as a desktop operating system for consumer level activity but this does not then mean that it’s the best platform for accomplishing research. Most of the personal computers and laptops sold today come with some version of Windows which then implies that most people have will have a degree of familiarity with it. You can also install things like SAS, SPSS, Systat but those are also available on Mac OS which we will get to soon enough. 1.5 What’s Wrong with Windows ? Nothing - as long as you like getting viruses and having to install a never ending series of software updates that seem to only compound the very problems that the updates alleged to address in the first place. For many people, it’s a fine Operating System for general office productivity. Information Technology managers LOVE it because it’s what “everyone else uses” even if that’s not a particularly good reason to do something. 1.5.1 Windows Security, Virus, and Performance Issues Do I REALLY need to convince you of this ? Windows has many security and virus problems to the extent that you MUST have Anti Virus tools installed that itself frequently prevents the installation of packages. Lots of IT managers will deprive the user of management permissions which requires the user to then go get help simply to install a package or tool. Default permissions are usually set to prevent most anything from being installed to make administration easier. There are some good IT people and I’m speaking in general here. It’s just that the typical Windows support person does not have a research or computation background so their interest in helping is probably going to be restricted to the “bread and butter” activities such as keeping the system healthy for running other MS products, accessing the web, email and calendering. On the other hand that should tell you everything you need to know - that industry and academia tend to view MS Windows as an office productivity setup so they rarely devote research support resources (but they should offer reasonable alternatives). 1.5.2 Windows Reboots and Updates Have you ever encountered the Windows Update screen that holds your computer captive, sometimes for hours, to install updates that no one other than Microsoft knows what they are actually for ? Well, they do have a site you can go to that describes the “fixes”. And then there are the times when the updates failed so you have to disable virus protection and repeat the process. And then you have to remember to re-enable virus protection. Windows Update is always there waiting for you to restart your system - and you will because inevitably your system will slow down for unknown reasons and of course the recourse is always to reboot. But Windows Update is ALWAYS watching: 1.5.3 Is Linux Really Better ? Bioinformatics thrives on open source tools the vast majority of which are built on Linux/UNIX. However, frameworks like R and Python will run on Windows so you CAN in fact do some things on Windows assuming you have control of your computer to install things. Linux offers an excellent package management system to help install specific tools (compilers and databases) you might need to develop code. It all depends on what you want to do. Most people who use Linux do not also run general office productivity apps on the same system although you could certainly do this. The typical Linux user is a scientist or developer who uses open source tools as part of their daily work. Let’s say that you read a research paper and see that there is a cool package or tool that was developed using open source so you want to run it. If you are lucky then it’s in the form a package that can be run under, for example, R in which case you can implement it on your Windows or Apple system. The problems come in when you want to “Scale Up” your activities and run tens, hundreds, or thousands of jobs at which point Linux is required. If you think that your computational activities will remain at a modest level then maybe you don’t need to learn or use Linux but in the era of “Big Data” that is something of a gamble. Amazon and Google both provide pre-made Linux images that come pre-installed with a number of tools likely to be of interest to a statistician or bioinformatics person. But you will still need to learn something about Linux administration be productive. However, a little knowledge will go a very long way 1.5.4 Cool Things About Linux See https://www.gotothings.com/unix/unix-features-and-advantages.htm for a long list some of which are summarized here: Portability: The system is written in high-level language making it easier to read, understand, change and, therefore move to other machines. The code can be changed and complied on a new machine. Customers can then choose from a wide variety of hardware vendors without being locked in with a particular vendor. Machine-independence: The System hides the machine architecture from the user, making it easier to write applications that can run on micros, mins and mainframes. Multi-Tasking: Unix is a powerful multi-tasking operating system; it means when a active task in in process, there can be a simultaneous background process working too. Unix handles these active and background threads efficiently and manages the system resources in a fair-share manner. Multi-User Operations: UNIX is a multi-user system designed to support a group of users simultaneously. The system allows for the sharing of processing power and peripheral resources, white at the same time providing excellent security features. Hierarchical File System: UNIX uses a hierarchical file structure to store information. This structure has the maximum flexibility in grouping information in a way that reflects its natural state. It allows for easy maintenance and efficient implementation. UNIX shell: UNIX has a simple user interface called the shell that has the power to provide the services that the user wants. It protects the user from having to know the intricate hardware details. 1.5.5 Linux is The Native Environment for Development. Linux provides support for almost any programming language you can think of. This includes Python, C/C++, Java, Perl, Ruby, FORTRAN, R, and many, many more. There are compilers and toolkits for using these languages on Windows assuming you can get the permission to install them and you know how to work the “Command Shell” in Windows. There are also IDEs (such as R Studio) that will run on Windows to insulate you from the command line but if you want to work on the domain of “Big Data” you will inevitably need to use the command line. Ironically, Microsoft, as a company, got its start by providing compilers for developing software products. They strayed from that mission (much to their financial advantage) to become the premier consumer operating system. Clearly this move worked out quite well for Bill Gates but the company took a direction guided by the needs of the mass market. Can’t say that I blame him for that. Anyway, bask in the glory of the splash screen for Microsoft Quick Basic which at one time was really cool if only for a short time. By then 1985 there were other compilers for “better” languages such as Pascal and C. 1.5.6 Command Line Knowledge is Next Level Stuff Linux offers the “Terminal Window” which is the primary interface to the operating system although Linux also provides a number of “nice” GUIs (Graphical User Interfaces) to interact with your files. Using the command line though becomes a very powerful way to create code and run it. Things like R Studio insulate you from having to do this but even that provides a way to interact with the “command line”. Windows command line support is terrible. In fact, Windows was designed to keep the end user AWAY from the command line which is massively inconvenient for those with a programming background or a general need to be productive with code. They do have something called “Powershell” which is aimed at Windows administrators but the end user can access it also. Note that if anyone ever asks you to do something to “edit the Windows Registry” then watch out ! 1.5.7 Is There Hope For Windows Users ? You can in fact get work done using Windows especially if you are using apps like R Studio, SAS, etc though at some point you will probably hit a permissions error or need to install something (e.g. LaTex) that requires a lot of work and troubleshooting just to do something that would take like 5 seconds on Linux or Apple OSX. Also remember that being able to scale up your analysis is more easily accomplished on Linux than Windows. This doesn’t mean that you should abandon your Windows laptop just that as you plan for a future career in research that might involve computation then you want to look at solutions that involve Linux. Both Google and Amazon offer excellent support for Linux instances as part of their cloud offerings. Even the Microsfot Azure cloud service offers Linux support as part of their service so they understand the importance of Linux in the scheme of things. 1.6 Is There a Middle Ground ? Yes there is. You can always use Windows to remotely access a Linux computer using tools such as Putty but this assumes that someone has setup that remote system for you. This is where Amazon and Google come in with their cloud services. In effect you are using your Windows laptop to access another computer so you don’t have to install very much on your local system. But this also assumes that you know something about Linux. 1.6.1 Virtualized Systems You can also use Virtualization software such as Oracle’s freely available Virtual Box that allows you to install Linux within a “fully virtualized computer” that is hosted within an application running on your Windows box. Just installing Virtual Box isn’t enough. You then have to pick the appropriate distribution of Linux you want to run and then install that. The advantage of this approach is that it provides access to a completely different operating system (in this case Linux) that appears to be an entire functioning machine that uses your system’s CD/DV, mouse, track pad, etc just like a “real” machine. You need to use key combinations to switch between your virtualized and host systems but that’s just part of the learning curve. VirtualBox runs on top of Apple, Windows, and Linux systems which means, for example, that you could run a copy of Windows on top of an Apple laptop. Using VirtualBox and fully exploring it’s power requires you to know about administration and management of the operating system in question which is why some people shy away from it though it remains a powerful tool for running multiple operating systems on one host. From a practical point of view you need to make sure that you desktop or laptop has enough memory and disk space to practically use VirtualBox since any operating systems you virtualize will share resources with your hardware. If you have around 8GB or more of RAM and at least 100GB free then you should be fine. But you also need to take into account any data sets you plan to process. 1.6.2 Dockers If using Virtualbox seems too much then consider using “Docker” technology which is a form of lightweight virtualization designed to give users easy access to specific services (e.g. other operating systems, analysis packages, databases,etc) without requiring a full on installation of something like VMWare or Virtualbox. Docker “images” are created by a community of interested users who then publish the images onto a repository or registry, usually Dockerhub, for use by others. Running them involves first installing the Docker software for your operating system and then issuing some commands that pulls down the image and executes it within a “container” that can interact with your local operating system. As an example, let’s consider the following scenario: I have R installed on my laptop and it has all the packages I need to do my work. I read some research paper that points me to a new R package that requires the latest version of R I have no interest in updating my local version of R just to get this package which might not ever do what I want it to do. What are my choices here ? Well, I could loin to a system somewhere that does have the latest version of R. This is where services like Amazon and Google come in handy since I can spin up my own Linux servers and experiment on them but let’s say I don’t want to spend any money on that. I could install Virtualbox and Linux on top of that but that’s too much work. I know, I’ll install the Docker software appropriate to my system (Apple) and run the latest version of R in a container. This executes the latest version of R in a protected container that in no way interferes with my existing version of R. I could also do this with another operating system altogether such as running Ubuntu Linux in a container on my Apple. Steves-MacBook-Pro:~ esteban$ docker run -it rocker/r-base Unable to find image &#39;rocker/r-base:latest&#39; locally latest: Pulling from rocker/r-base 71c170c5dae2: Pull complete 1b76173b98c5: Pull complete 1b00be862536: Pull complete c48ed365264c: Pull complete b2f3e26a95d6: Pull complete bdb9fc7fc7fb: Pull complete Digest: sha256:0589141389482d3211dbc9ccef20e1b426cc8ed7644c2ef0f60862becf3bea4e Status: Downloaded newer image for rocker/r-base:latest R version 3.5.3 (2019-03-11) -- &quot;Great Truth&quot; Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type &#39;license()&#39; or &#39;licence()&#39; for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type &#39;contributors()&#39; for more information and &#39;citation()&#39; on how to cite R or R packages in publications. Type &#39;demo()&#39; for some demos, &#39;help()&#39; for on-line help, or &#39;help.start()&#39; for an HTML browser interface to help. Type &#39;q()&#39; to quit R. &gt; 1.6.3 Anaconda From the Anaconda page: Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. Conda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language. Conda is tool that allows you to manage Python and (to an extent) R installations. It insulates you from the details of installing Python versions and allows you to maintain multiple versions of Python on one system should you need that type of environment. An advantage of this is that you can then easily install something like Jupyter Notebooks that facilitate development of Python code. 1.7 Apple OSX / macOS ? Hmm. Isn’t Apple a proprietary system ? Why yes it is but Apple laptops, unbeknownst to most people, runs a form of UNIX natively ! It’s been there for some time. Just find the “Terminal” application. Start it up and you will see something like the following. The interesting thing about this is that the GUI (The Graphical User Interface) that you interact with is itself a process that runs on top of UNIX ! The reason I am advocating Apple is not because I’m fond of paying more for a computer (see comparison below) but because it’s much close to Linux by far than MS Windows which is good news for anyone who does BioMedical or Bioinformatics work - more generally anything opensource. 1.7.1 The Dark Years of Apple It’s way outside of the scope of this discussion to explain the history of Apple products and it’s various operating system but for a number of years (mostly up to an including 2001) it suffered from stability and performance issues due to its general inability to effectively manage multiple processes which would then lead to unexpected crashes and screens like the following. This was so common that people called it The Bomb. The Microsoft equivalent was called “The Blue Screen of Death” aka BSOD. And there was a time when both Apple and Microsoft battled it out for the crown of unexpected crashes. Modern Apple systems do not crash nearly as often as the systems of old but occasionally they can, (due usually to hardware issues), and when it happens, one usually observes what is known as The Spinning Beach ball or The Beach ball of Death. In reality this can be more easily addressed than The Bomb which was more strongly associated with Max OS 9 but it’s still a pain. Take comfort in the knowledge that it happens far less frequently than a decade ago. This is due to the fact that the Apple Operating System is running on top of UNIX which handles processes very efficiently. 1.7.2 Apple OSX vs Previous Apple Operating Systems Around 2001 Apple decided to base it’s interface on a UNIX type of operating system. This is something of a simplification but this introduced enhanced stability for the platform which in turn made development for OSX more uniform than before. What this means for you is that you have access to UNIX. This does not mean that you will instantly know how to use it but it’s there if you want it. Check out this graphic which presents the history of the UNIX operating system and Apple’s relation to that development tree: Don’t get me wrong - simply running UNIX does not guarantee enhanced performance for applications that don’t take advantage of it but the Apple OSX User interface is very well integrated with UNIX which provides, at least in my experience, a much smoother experience overall with fewer crashes. 1.7.3 Let’s Go Shopping ! Of course I understand the appeal of Windows-based laptops and desktops - they are quite inexpensive when compared to Apple products. For example check out the price on this Dell Inspiron. It also includes a 15.6 inch screen, a camera, fingerprint reader, touch pad, DVD, a 512GB Solid State Drive, and a bunch of USB and HDMI ports. When looking at the comparable Apple there is a drastic price difference though keep in mind that the CPU is very good as is the display. But still there is a financial consideration to be made. But consider that the hardware and software are designed and implemented by the same company whereas with Windows based laptops the hardware can be designed by any number of vendors. For me, the performance and the relatively headache-free experience of not having viruses is worth it. Especially since I can develop very easily. Where this all falls apart is that with Window-based hardware one can usually add storage much cheaper. There are a number of ways to manage the installation of UNIX tools on your Apple laptop (or desktop) but many of the tools you would want access to at the command line are already installed. Apple offers something called XCode which provides access to compilers for development. Obviously they would like for you to develop applications that run natively on OSX but this isn’t at all a requirements. But since it’s all UNIX under the hood you can have compilers like gcc and gfortan. 1.7.4 Managing Software Locally Usually you want to install one or more programming environments to facilitate analysis and the possible development of packages for eventual sharing with others. So it’s common to install things like RStudio and perhaps Python along with commercial tools like SAS and MATLAB. This is possible and if done with care you should be okay. Specifically make sure you have plenty of hard drive space and plenty of RAM (at least 8GB or more) to realistically do any work. 1.7.5 RStudio This is usually a very straightforward installation process with few surprises and for the most part RStudio works very well. Where it doesn’t work well is when you need to install packages that in turn require some other version of R than the one you have installed. 1.7.5.1 BioConductor BioConductor is an add on environment for R that is dedicated to managing and analyzing data types of interest to bio medical investigators. However, it brings its own set of problems in that you have to use a special installer that is independent of the typical R package installer. This is because BioConductor imposes a specific structure on top of the data to ensure that downstream BioConductor packages can easily “digest” the incoming data. BioConductor is also highly dynamic and packages are updated frequently which can lead to version collisions. In recognition of some of these problems the BioConductor team recently introduced a new tool to manage BioConductor installations that allows one to install multiple versions of BioConductor alongside each other with interference. This is a convenience worth having if you plan to do a lot of bioinformatics work. To install BioConductor do the following within an active R installation: # Get the installer if you don&#39;t already have it install.packages(&quot;BiocManager&quot;) # This will install most of the basic packages BiocManager::install() # Find available Bioconductor packages BiocManager::available() 1.7.5.2 Python Python comes by default on Apple systems. The newer systems come with Python version 3 as general support for Python 2 is going away. This has nothing to do with Apple. It’s a Python community kind of thing. So the bottom line is that if you need to have Python access you have it by default BUT it’s best not to mess with the default installation since Apple OSX relies on Python to do some things administratively. At least this is what I have found. What I generally do is use the Conda installer to manage different versions of Python on my Apple. You can also do this on Linux or MS Windows ! There is a smaller version of Conda called Miniconda which has a smaller installation foot print so you could start with that. That’s the only difference. Both of these allow you to install multiple versions of Python in a way that insulates those respective versions from each other. This is great for evaluating, testing, and running various Python modules that might require differing versions of Python. Conda provides access to a number of pre-built channels that contain pre-built modules (such as numpy, scipy, scikit, matplotlib) which means all you have to do is install them without concern for downloading and compiling them yourself. This is a true convenience in that you almost never have to deal with version conflicts or compile modules yourself. Steves-MacBook-Pro $ conda env list # conda environments: # base * /Users/esteban/anaconda3 "],
["virtualization.html", "Chapter 2 Virtualization 2.1 Virtual Box 2.2 Using Virtualization Farms 2.3 Running RStudio on Amazon 2.4 Accessing Your Instance 2.5 Getting Data Into Your Instance 2.6 Accessing S3 from EC2 Instances 2.7 Shiny Server 2.8 Making an AMI", " Chapter 2 Virtualization Virtualization can help deal with the problem of having to use multiple operating systems although it requires some level of operating system administration knowledge to fully exploit. At the most basic level a person can install a Virtualization package onto a desktop or laptop and then host one or more “guest” operating systems on top of that which will appear to be running “natively” on your hardware. The “guest” opersting system will use the hard drive, memory, keyboard, mouse, touch pad just as it was running on its own. OF coruse a limistation of this approach is that the guest operating system will be competing with your host operasting system for resources so you will need to appropriately allocate resources to the guest OS when you set it up to ensure decent performance. The most common virtualization setup I see are people with Apple laptops wanting to run MS Windows on their Apple. Apple even has a product for this called Parallels which cost around $80 though open soruce tools like Virtualobox can also provide accress There is a also a product called VMWare which can do this so there are some options. Of course there are Windows users who want to run Linux on top of their Windows OS. You could also be a Windows user who wants to run a protected copy of Windows on top of your “host” Windows to do some experiments without impacting your natively installed copy. 2.1 Virtual Box There is a free virtualization tool called VirtualBox that can be downloaded for free for use on Windows, Apple, or Linux hardware. It works pretty well and the price is right. Keep in mind that any operating systems you install within Virtual Box will share the hard drive and memory of you desktop or laptop. Also consider that if you wanted to run MS Windows as a guest operating system that you would need to get a license for it. That is, just because you have the ability to run Windows as a guest OS does not mean that it would be free. You are still bound by the license terms that apply to any version of Windows. With Linux of course it’s free. Here is a screenshot of my Apple laptop where I used a copy of Virtualbox to host an installation of Mint Linux which is a small version of Linux. 2.2 Using Virtualization Farms You have probably heard of Amazon. It provides virtualization capabilities at a very large scale as well as many other services that exploit this ability. Both Google and Microsoft also have this capability but Amazon has a head start. 2.3 Running RStudio on Amazon I’ll assume that you know something about getting into Amazon so I’ll jump right in. For this example I will use the Ubuntu 18 which is the newest although typically one would use a version that has been around for a few years. For Ubuntu that would be version 16 or even 14. The advantage of using something older is that there is more Google information on various problems that might emerge. I’m picking Ubuntu 18 because I wanted to see how it differed, if at all, from the previous versions. There are some oddities but nothing major. 2.3.1 Why Ubuntu ? I’m using Ubuntu because it is the most friendly Linux OS for bioinformatics tools although almost any known version of Linux could be used - it’s just that you might have to use different package managers. CentOS is a popular alternative and there are a number of other “distros”. 2.3.2 Start An Instance Login to your Amazon AWS account and select something like a t2.medium instance with around 50-100GB of space. This will vary with your project needs but this is just an example. 2.3.3 Add Some Storage Add some storage to the instance. If you find yourself wanting to add a lot of storage here then you should consider adding a separate volume. That way when we later create an AMI (Amazon Machine Image) you don’t make an image out of a very large volume which will in turn cost more money. 2.3.4 Get A Key As always you will need a key to get into your instance. Either select an existing key or create a new one. 2.3.5 Installing R Once you have your instance up and running let’s install the latest version of R. Strictly speaking we don’t have to install the absolute latest version. If we make no effort to get the latest version then we will get whatever version of R was current at the time the release was built. You can refer to this page for specific information on how to get the latest R release. With Linux there are a number of supporting packages that you will need to get to install packages like tidyverse. The same is true with the RStudio Server software. $ sudo apt install apt-transport-https software-properties-common $ sudo apt install build-essential $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 $ sudo add-apt-repository &#39;deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/&#39; $ sudo apt install libcurl4-openssl-dev libssl-dev libxml2-dev $ sudo apt update $ sudo apt install r-base So for now just accept that you will have to install some packages in support of getting the latest version of R. Knowing something about system administration is a good thing particularly as it relates to the package manager “apt-get”. You don’t have to be an expert but it’s important to know how to add, remove, and update packages. $ R --version R version 3.5.3 (2019-03-11) -- &quot;Great Truth&quot; Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see http://www.gnu.org/licenses/. # Here you can install tidyverse. This will compile the code for tidyverse $ R &gt; install.packages(&quot;tidyverse&quot;) 2.3.6 Installing RStudio Server The next step involves installing the RStudio server software which requires some supporting files: $ sudo apt-get -y install gdebi-core Next we want to go to the Downloads page for RStudio to get a version of RStudio server that is approved for use with Ubuntu 18. $ wget https://download2.rstudio.org/rstudio-server-1.1.463-amd64.deb $ sudo gdebi rstudio-server-1.1.463-amd64.deb Created symlink /etc/systemd/system/multi-user.target.wants/rstudio-server.service → /etc/systemd/system/rstudio-server.service. ● rstudio-server.service - RStudio Server Loaded: loaded (/etc/systemd/system/rstudio-server.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2019-04-02 02:33:57 UTC; 1s ago Process: 32618 ExecStart=/usr/lib/rstudio-server/bin/rserver (code=exited, status=0/SUCCESS) Main PID: 32628 (rserver) Tasks: 3 (limit: 4704) CGroup: /system.slice/rstudio-server.service └─32628 /usr/lib/rstudio-server/bin/rserver Apr 02 02:33:57 ip-172-30-1-144 systemd[1]: Starting RStudio Server... Apr 02 02:33:57 ip-172-30-1-144 systemd[1]: Started RStudio Server. $ sudo rstudio-server verify-installation 2.4 Accessing Your Instance If the installation went as expected then fire up your favorite web browser and point it at the public IP number associated with your instance: http://18.212.63.117:8787 This probably won’t work unless as part of the instance setup you thought to allow inbound connections on port 8787. Most people forget but that’s okay. You can go back and fix this by looking at your instance details and identifying your security group. After that you can change the inbound rules. Now you should be able to access the web interface for RStudio with your browser. The question then becomes what userid should you use ? Well you could add one or more userids and create passwords. The installation of RStudio is able to accept userid and password information for users on your system. Let’s change the password on the default ubuntu password so we can get in. To do this go back to your terminal where you logged into your instance. Use the passwd command to change the password. Afer you do this then you can log into your RStudio instance $ sudo passwd ubuntu Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully 2.5 Getting Data Into Your Instance You now have access to a remote instance of RStudio but you need to figure out how to get data into it. You could use something like the scp or sftp command to upload data from your laptop into your instance. That’s one way to do this. Let’s say we have a local folder named test_data that we want to copy up to our instance. $ scp -i ~/.ssh/bios560.pem -r test_data ubuntu@18.212.63.117:/home/ubuntu bird 100% 13 0.5KB/s 00:00 cat 100% 0 0.0KB/s 00:00 dog 100% 15 0.6KB/s 00:00 2.6 Accessing S3 from EC2 Instances Another way to get data into your instance is by copying it over from your S3 buckets. S3 is object based storage that is very cheap and you can stock pile data over there until you are ready to analyze it. But you need to know how to copy it over. There are a couple of ways to do this. The first is that you could assign a role that would allow your instance to access S3 as you create the instance. Or you could make a change once it is up and running as follows: The idea is to assign a role that will allow your EC2 instance to “read” your S3 buckets. So you can now access your S3 resources using the AWS CLI (Command Line Interface). You can do this from your Terminal window. For the S3 CLI documentation go here $ sudo apt-get install awscli $ aws s3 ls 2012-11-04 02:23:18 R_Stuff 2014-04-24 18:53:17 bimcore_final_figures 2015-04-08 18:29:09 cm-89cc48c8931698be927013c2775cf11e 2018-11-30 03:31:29 juniferous 2014-11-03 21:44:19 pittaraujo13 2014-04-24 18:39:45 steviep42bitbucket $ aws s3 ls s3://juniferous PRE AWSLogs/ 2018-12-05 20:19:04 97 error.html 2018-12-04 21:57:39 13 healthcheck.html 2018-12-05 20:19:03 732 index.html 2019-04-01 21:37:18 63566904 rstudio-server-1.1.463-amd64.deb 2.7 Shiny Server You can install Shiny Server also using the directions found here. Login to your instance you created previously. The following command will result in lots of messages. It installs the shiny server along with the pre requisites. You might encounter some warning messages. $ sudo su - \\ -c &quot;R -e \\&quot;install.packages(&#39;shiny&#39;, repos=&#39;https://cran.rstudio.com/&#39;)\\&quot;&quot; So now download the actual server code and install it. You should not encounter any errors when doing this. The last thing you should see is a message indicated that the server has started. By default, your server will be running on port 3838 so make sure your instance firewall / security group will allow access on that port. $ sudo apt-get install gdebi-core $ wget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.9.923-amd64.deb $ sudo gdebi shiny-server-1.5.9.923-amd64.deb So the default location for the server apps are in /srv/shiny-server/sample-apps So point your browser at whatever your IP number is followed by /sample-apps/rmd As an example **http://3.87.30.168:3838/sample-apps/rmd/** $ ls -lR .: total 8 drwxrwxr-x 2 root root 4096 Apr 3 15:42 hello drwxrwxr-x 2 root root 4096 Apr 3 15:42 rmd ./hello: total 8 -rw-rw-r-- 1 root root 642 Sep 11 2018 server.R -rw-rw-r-- 1 root root 494 Sep 11 2018 ui.R ./rmd: total 4 -rwxrwxr-x 1 root root 484 Sep 11 2018 index.Rmd 2.8 Making an AMI One thing you can do to help yourself is to create an AMI (an Amazxon Machine Instance) that will save all the work you have in a way that allows you to share it with others. They can use your image as a reference from which to boot a machine so they don’t have to go through all the headaches you did when setting things up, installing packages, and generally trying to get things to work. This is easy to do. So now we need to provide some information about our image Then we get confirmation After it gets created you can change it’s permissions to public so that others may access it when launching their machine. "],
["dockers-1.html", "Chapter 3 Dockers 3.1 Scenarios 3.2 Differences From VMs 3.3 Docker Terminology 3.4 RShiny 3.5 Making Your Own Image 3.6 There is Another Way", " Chapter 3 Dockers If using Virtualbox seems too much then consider using “Docker” technology which is a form of lightweight virtualization designed to give users easy access to specific services (e.g. other operating systems, analysis packages, databases,etc) without requiring a full on installation of something like VMWare or Virtualbox. Docker “images” are created by a community of interested users who then publish these images onto a repository or registry, usually Dockerhub, for use by others. Running them involves first installing the Docker software for your operating system and then issuing some commands that pulls down the image and executes it within a “container” that can interact with your local operating system. 3.1 Scenarios As an example, let’s consider the following scenarios: I’m on a Windows or an Apple laptop and I need an Ubuntu OS to run some software I read about that will only run on Linux. I don’t want to install Virtualbox I have R installed on my laptop and it has all the packages I need to do my work. I read some research paper that points me to a new R package that woild require me to update my version of R but I really don’t want to do this. I am developing an R (or Python) package and I want to share it with my colleagues without requiring them to do install a lot of pre-requisites. I am developing software that has a web server, database, and a front end user facing application. I want to develop this all locally without having to involve system administrators and other people. Docker solves these problems. It’s not the only solution but it is becoming incresasingly popular to develop and design applications to run inside of light weight “containers” that sit on top of you OS without interfering with it. Let’s look at scenario number one again: I’m on a Windows or an Apple laptop and I need an Ubuntu OS to run some software I read about that will only run on Linux. I don’t want to install Virtualbox Here is the Docker solution. This is using my Apple laptop. $ docker run -it ubuntu Unable to find image &#39;ubuntu:latest&#39; locally latest: Pulling from library/ubuntu 898c46f3b1a1: Pull complete 63366dfa0a50: Pull complete 041d4cd74a92: Pull complete 6e1bee0f8701: Pull complete Digest: sha256:017eef0b616011647b269b5c65826e2e2ebddbe5d1f8c1e56b3599fb14fabec8 Status: Downloaded newer image for ubuntu:latest root@85455cbcf54d:/# ls bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@85455cbcf54d:/# uname -a Linux 85455cbcf54d 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux root@85455cbcf54d:/# I have R installed on my laptop and it has all the packages I need to do my work. I read some research paper that points me to a new R package that woild require me to update my version of R but I really don’t want to do this. Steves-MacBook-Pro:~ esteban$ docker run -it rocker/r-base Unable to find image &#39;rocker/r-base:latest&#39; locally latest: Pulling from rocker/r-base 71c170c5dae2: Pull complete 1b76173b98c5: Pull complete 1b00be862536: Pull complete c48ed365264c: Pull complete b2f3e26a95d6: Pull complete bdb9fc7fc7fb: Pull complete Digest: sha256:0589141389482d3211dbc9ccef20e1b426cc8ed7644c2ef0f60862becf3bea4e Status: Downloaded newer image for rocker/r-base:latest R version 3.5.3 (2019-03-11) -- &quot;Great Truth&quot; Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type &#39;license()&#39; or &#39;licence()&#39; for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type &#39;contributors()&#39; for more information and &#39;citation()&#39; on how to cite R or R packages in publications. Type &#39;demo()&#39; for some demos, &#39;help()&#39; for on-line help, or &#39;help.start()&#39; for an HTML browser interface to help. Type &#39;q()&#39; to quit R. &gt; Dockers are far more light weight than VMs. Think of it this way: If you have 30 Docker containers that you want to run, you can run them all on a single virtual machine (or real machine). To run 30 virtual machines, you’ve got to boot 30 operating systems with at least minimum resource requirements available before factoring the hypervisor for them to run on with the base OS. https://blog.codeship.com/why-docker/ Containers work a little like VMs, but in a far more specific and granular way. They isolate a single application and its dependencies—all of the external software libraries the app requires to run—both from the underlying operating system and from other containers. https://www.infoworld.com/article/3310941/why-you-should-use-docker-and-containers.html 3.2 Differences From VMs Here is a diagram of what a Virtual Server might look like. Notice that there is a “Hypervisor” on top of the host that enables you to install and maintain multiple operating systems. See this link from which the following graphics were taken for more discussion for more discussion And here is the diagram for a container / Docker approach: Lastly, we have a diagram of the Docker Universe. What this means to you is that you will install the Docker software for your computer which then executes a daemon (a persistent process) that can communicate with Docker Hub to identify and obtain existing images that you can run locally. You can develop images and then push them up to Docker Hub (as you would with GitHub for general source code) for others to use. 3.3 Docker Terminology The terminology is pretty easy and straightforward. Docker hub is a free (for the most part) registry service to maintain Docker images thats can be pulled down to your computer. You can then run the images inside of containers. There are commands to develop your own images that can be executed in containers which can talk to each other where desired. Of course there are commands that allow you to find, execute, and maintain images and containers as well as develop them. Go to the Docker site and install the code for your platform. 3.3.1 Images You can pull an image. Once it’s been downloaded it will stay local unless you make and effort to remove it. Think of an image as read-only package that has whatever you need to do something. # Show current docker activity on your machine $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES # Show all activity (past and present) $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85455cbcf54d ubuntu &quot;/bin/bash&quot; About an hour ago Exited (0) 2 minutes ago keen_wright $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 94e814e2efa8 3 weeks ago 88.9MB You can search for images based on what they do. $ docker search python NAME DESCRIPTION STARS OFFICIAL AUTOMATED python Python is an interpreted, interactive, objec… 4026 [OK] django Django is a free web application framework, … 805 [OK] pypy PyPy is a fast, compliant alternative implem… 175 [OK] kaggle/python Docker image for Python scripts run on Kaggle 114 [OK] frolvlad/alpine-python3 The smallest Docker image with Python 3.5 (~… 96 [OK] Let’s look for a images that have the R tidyverse, Rstudio, and Shiny. There is a project called “rocker” that has almsot all the cool R stuff you would want in the form of a Docker image. $ docker search rocker | grep shiny rocker/shiny-verse Rocker Shiny image + Tidyverse R packages. U… $ docker search rstudio NAME DESCRIPTION STARS OFFICIAL AUTOMATED rocker/rstudio RStudio Server image 243 [OK] opencpu/rstudio OpenCPU stable release with rstudio-server (… 29 [OK] rocker/rstudio-stable Build RStudio based on a debian:stable (debi… 14 [OK] So someone went ahead and made images with all that you need to run these packages. When we run this we have to tell the image what password we want to provide for the rstudio password. The rm option tells docker to remove the image once we are done using the image. $ docker run --rm -e PASSWORD=&#39;steve&#39; -p 8787:8787 rocker/verse That’s great and all but how then does the Docker image interact with the local filesystem on your laptop ? First, let’s stop the running container docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 292adbfaa2e1 rocker/verse &quot;/init&quot; 3 hours ago Up 3 hours 0.0.0.0:8787-&gt;8787/tcp optimistic_cori $ docker stop optimistic_cori optimistic_cori I’m on an Apple so I want my home folder to be available on the Rstudio server under the directory as /home/esteban $ docker run --rm -v /Users/esteban:/home/esteban -e PASSWORD=&#39;steve&#39; \\ -p 8787:8787 rocker/verse 3.4 RShiny Note that the “rocker” project has lots of Docker stuff relative to R. Check out https://github.com/rocker-org/shiny for more information. So let’s see within my home folder I have a folder with Server.R and UI.R files. This is a very simple application. Let’s look at the content of the files: $ cd appdir/ $ ls Server.R UI.R $ cat Server.R # Define server logic required to print &quot;Hello World&quot; when button is clicked shinyServer(function(input, output) { # Create action when actionButton is clicked observeEvent(input$Print_Hello,{ # Change text of Server_Hello output$Server_Hello = renderText(&quot;Hello world from server side&quot;) }) }) $ cat UI.R library(shiny) # Define UI for application print &quot;Hello world&quot; shinyUI( # Create bootstrap page fluidPage( # Paragraph &quot;Hello world&quot; p(&quot;Hello world&quot;), # Create button to print &quot;Hello world&quot; from server actionButton(inputId = &quot;Print_Hello&quot;, label = &quot;Print_Hello World&quot;), # Create position for server side text textOutput(&quot;Server_Hello&quot;) ) ) Now let’s run a Shiny Server $ docker run -p 3838:3838 -v /Users/esteban/appdir/:/srv/shiny-server/ rocker/shiny Nezt let’s lood the Shiny App 3.5 Making Your Own Image The last thing we will discuss is how to build upon an existing image to create your own image that you could then publish to Docker Hub to distribute to others. Let’s use one of the rocker images as a base for installing some specific packages. This will be a trivial example but you could make significant additions to an image if you wanted to. The advantage of this approach is that you don’t have to build everything from scratch. For example, let’s Run the rocker/verse image Install a package (e.g. data.table) Commit the changes to the image Give it a new name Push our new image up to Docker hub. # This will launch an Rstudio session from which we can install the data.table package $ docker run --rm -e PASSWORD=&#39;steve&#39; -p 8787:8787 rocker/verse So now we will commit changes to this image since the next time we start it we would like for it to have the data.table package already installed. This is a basic example but imagine if we had installed a lot of packages. We wouldn’t want to do this every time. # Get the Container ID $ docker ps | awk &#39;{print $1,&quot; &quot;,$2}&#39; CONTAINER ID 439385c073f8 rocker/verse # Commit the changes we made to it (i.e. the package install) $ docker commit 439385c073f8 steviep42/r-rocker-datatable sha256:f75857b7fb0060c31cbead22fdc6e37dc341bcea111b570d990d1b31cb88d67f (base) Steves-MBP:~ esteban$ docker push steviep42/r-rocker-datatable The push refers to repository [docker.io/steviep42/r-rocker-datatable] 15b389285a52: Pushed d466632838cf: Mounted from rocker/verse 395ecd3ce103: Mounted from rocker/verse 23fc0d314261: Mounted from rocker/verse 9142a472c042: Mounted from rocker/verse 2b40fab709d4: Mounted from rocker/verse f453344be0c3: Mounted from rocker/verse 0fef5c3c51c6: Mounted from rocker/verse e8481c9f7465: Mounted from rocker/shiny fbb641a8b943: Mounted from rocker/tidyverse latest: digest: sha256:368dd8950a433cc14ba799f6c4a39b812eff904b670563fd33ecad64d8a61c85 size: 2422 3.6 There is Another Way We can create what is known as a Dockerfile which can contain the instructions to do what we did above except in a more convenient way. Make a folder called build. Note that this is arbitrary. $ mkdir build $ cd build # This next step will create a file called Dockerfile $ vi Dockerfile Give your Docker file the following contents FROM rocker/verse:latest RUN echo &#39;install.packages(c(&quot;data.table&quot;), \\ repos=&quot;http://cran.us.r-project.org&quot;, \\ dependencies=TRUE)&#39; &gt; /tmp/packages.R \\ &amp;&amp; Rscript /tmp/packages.R EXPOSE 8787 Next, we can build a new image that is based on rocker/verse:latest. This is cool because we don’t have to go to the trouble of installing all the underlying packages used to make that image. We just build on what we already have. $ docker build -t steviep42/test . Sending build context to Docker daemon 2.048kB Step 1/3 : FROM rocker/verse:latest ---&gt; ed004402fcbc Step 2/3 : RUN echo &#39;install.packages(c(&quot;data.table&quot;), repos=&quot;http://cran.us.r-project.org&quot;, dependencies=TRUE)&#39; &gt; /tmp/packages.R &amp;&amp; Rscript /tmp/packages.R ---&gt; Using cache ---&gt; 518dd192faf4 Step 3/3 : EXPOSE 8787 ---&gt; Using cache ---&gt; ddf2c7ed59dc Successfully built ddf2c7ed59dc Successfully tagged steviep42/test:latest Now we can test it out: docker run -e PASSWORD=&#39;steve&#39; -p 8787:8787 steviep42/test "],
["text-mining.html", "Chapter 4 Text Mining 4.1 Recap 4.2 Unstructured Data 4.3 Structured Data 4.4 Hybrids 4.5 Information Retrieval vs Information Extraction 4.6 Web Scraping Revisited 4.7 Exploring The Text 4.8 Term Frequency 4.9 Tf and Idf 4.10 Sentiment", " Chapter 4 Text Mining 4.1 Recap In my last lecture I talked about using the rvest package to pull data from the Internet. This is useful because there is lots of information out there on the Internet that you could download and analyze. It can also be a pain to clean data and get it into a format you would like but that’s life and there isn’t a lot you can do about that. Every 60 seconds on Facebook: 510,000 comments are posted and 293,000 statused are updated Facebook Photo uploads total 300 million per day Worldwide, there are over 2.32 billion active Facbook users as of December 31, 2018 Every second, on average, around 6,000 tweets are tweeted on Twitter 1,332,433 Publications in Pubmed for the year 2018 Here are the top Twitter accounts being followed. A tweet from any of these people can represent a lot of money ! There are people who try to figure out just how much. Sources: https://zephoria.com/top-15-valuable-facebook-statistics/ http://www.internetlivestats.com/twitter-statistics/ http://dan.corlan.net/cgi-bin/medline-trend?Q= 4.2 Unstructured Data Much of the text information found in these sources is unstructured meaning that the content is a narrative, a collection of phrases, or maybe social media posts that might involve domain specific references or a form of slang. Not surprisingly, it can be hard to get meaningful information from text. Unstructured data is useful in that can capture many contexts or points of views in response to a question or situation. It’s also more “natural” for humans to speak and write in unsrtuctured terms. http://www.triella.com/unstructured-data/ Some unstructured text does conform to an ontology, vocabulary, or some form of domain approved terminology that can simplify knowledge extraction but there are always ambiguities. For example consider a body (aka “corpus”) of medical information related to Nephrology. The following terms might be used to refer to the same thing: Chronic renal impairment Chronic kidney disease CKD Kidney failure Chronid renal failure CRF Chronic renal failure syndrome Renal insufficiency eGFR 44 Source: http://healtex.org/wp-content/uploads/2016/11/I4H-Healtex-tutorial-part1-optimised.pdf Unless someone knows that these all map to the same idea then downstream analysis might lead to confusing or diluted results. 4.3 Structured Data Stuctured data is easier to work with in the sense that it is frequently numeric in nature so it lends itself well to statistical analysis and use in building predictive models. Some structured data is standalone meaning you can download it or obtain it as a .CSV file or from a database (e.g. CDC) and begin to work with the data immediately upon receipt. However, structured data does NOT have to be numeric. For example, consider demographic information. As long as a patient tracking system conforms to a standard then one’s race (and the label used to describe that) would ususally not change. Lists of personal allegies and medications are ususally structured although they might change over time. Smoking status (“yes” or “no”) is structured. Asking someone to describe their life time struggles with trying to stop smoking would not be structured information. 4.4 Hybrids Consider a Pubmed publication. There is a lot of textual information that is supplemented by graphs, tables, and (hopefully) link outs to raw data. Of course the tables themselves represent a form of structured data in aggregate form so you maybe you would want to parse tor extraxt them from the text. Also consider that Clinician notes and letters are ususally in narrative form but might well include structured info also. The take home info here is that there is by far more unstructured data than structured but there is lots of cool stuff to be learned from unstructured information. So don’t let that stop you from trying to apply text mining methods for understanding collections of documents. 4.5 Information Retrieval vs Information Extraction Searching for data is different from extracting meaning from the results returned from a search. If a search engine is good it will return relevant results in response to our query. A bad search engine will return lots of “false positives”. The idea of relevance can be expressed using a number of mathematically computed ratios such as precision (aka positive predictive value) and recall (aka sensitivity). We’ll dig more into these later. https://en.wikipedia.org/wiki/Precision_and_recall So for this lecture we’ll assume that we have some meaningful results already and work with the text that we have (or will) obtain from using some web scraping techniques we used in a previous lecture. 4.6 Web Scraping Revisited So I just said that we would be focusing on how to compute on the data instead of how to retrieve it but it’s important to review some basic techniques to refresh your memory and up your R game a little bit. Let’s get some speeches given by former President Obama. This is easy to do. Look at this URL http://www.obamaspeeches.com/ We can get all the speeches from this page if we want to using the rvest package. It’s tedious and takes some time to get things to work correctly but it’s not intellectually challenging. Data scraping and cleaning are two of the more common tasks when working with unstructured text. There is no way around it really. Most projects start out with lots of enthusiam until the data starts rolling in and everyone realizes that there are problems with formatting, etc. Anyway, we’ll get a list of all the links from the page itself. Well all the links that refer to a speech. library(rvest) url &lt;- &quot;http://www.obamaspeeches.com/&quot; links &lt;- read_html(url) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) links[c(1:2,110:114)] ## [1] &quot;/P-Obama-Inaugural-Speech-Inauguration.htm&quot; ## [2] &quot;/E11-Barack-Obama-Election-Night-Victory-Speech-Grant-Park-Illinois-November-4-2008.htm&quot; ## [3] &quot;003-John-Lewis-65th-Birthday-Gala-Obama-Speech.htm&quot; ## [4] &quot;002-Keynote-Address-at-the-2004-Democratic-National-Convention-Obama-Speech.htm&quot; ## [5] &quot;001-2002-Speech-Against-the-Iraq-War-Obama-Speech.htm&quot; ## [6] &quot;http://www.cafepress.com/obamaquotes&quot; ## [7] &quot;http://www.amazon.com/INSPIRE-NATION-Electrifying-Speeches-Inauguration/dp/0982100531/?tag=obama-speeches-com-20&quot; So there are about 114 speeches but the last couple of elements don’t appear to relate to the speeches so we’ll pull them from the list: links[c(length(links)-3):length(links)] ## [1] &quot;002-Keynote-Address-at-the-2004-Democratic-National-Convention-Obama-Speech.htm&quot; ## [2] &quot;001-2002-Speech-Against-the-Iraq-War-Obama-Speech.htm&quot; ## [3] &quot;http://www.cafepress.com/obamaquotes&quot; ## [4] &quot;http://www.amazon.com/INSPIRE-NATION-Electrifying-Speeches-Inauguration/dp/0982100531/?tag=obama-speeches-com-20&quot; I’m going to formalize this a bit more and make it better because it turns out that some of the speech links on that page are duplicated. Besides, I also want to get back the full link so I can access it and download the text. linkScraper &lt;- function(baseurl=&quot;http://obamaspeeches.com&quot;) { # # Function to grab links from the Obama Speech site # http://obamaspeeches.com/ # suppressMessages(library(rvest)) url &lt;- &quot;http://obamaspeeches.com/&quot; links &lt;- read_html(url) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) links &lt;- gsub(&quot;/&quot;,&quot;&quot;,links) cleaned_links &lt;- links[-grep(&quot;http&quot;,links)] cleaned_links &lt;- cleaned_links[!duplicated(cleaned_links)] retlinks &lt;- paste(&quot;http://obamaspeeches.com&quot;,cleaned_links,sep=&quot;/&quot;) return(retlinks) } obamalinks &lt;- linkScraper() obamalinks[3:10] ## [1] &quot;http://obamaspeeches.com/E-Barack-Obama-Speech-Manassas-Virgina-Last-Rally-2008-Election.htm&quot; ## [2] &quot;http://obamaspeeches.com/E10-Barack-Obama-The-American-Promise-Acceptance-Speech-at-the-Democratic-Convention-Mile-High-Stadium--Denver-Colorado-August-28-2008.htm&quot; ## [3] &quot;http://obamaspeeches.com/E09-Barack-Obama-Final-Primary-Night-Presumptive-Democratic-Nominee-Speech-St-Paul-Minnesota-June-3-2008.htm&quot; ## [4] &quot;http://obamaspeeches.com/E08-Barack-Obama-North-Carolina-Primary-Night-Raleigh-NC-May-6-2008.htm&quot; ## [5] &quot;http://obamaspeeches.com/E07-Barack-Obama-Pennsylvania-Primary-Night-Evansville-Indiana-April-22-2008.htm&quot; ## [6] &quot;http://obamaspeeches.com/E06-Barack-Obama-AP-Annual-Luncheon-Washington-DC-April-14-2008-religion-guns-pennsylvania.htm&quot; ## [7] &quot;http://obamaspeeches.com/E05-Barack-Obama-A-More-Perfect-Union-the-Race-Speech-Philadelphia-PA-March-18-2008.htm&quot; ## [8] &quot;http://obamaspeeches.com/E04-Barack-Obama-March-4-Primary-Night-Texas-and-Ohio-San-Antonio-TX-March-4-2008.htm&quot; Now let’s get the actual speech content. Not only will we get the speech content but we will keep track of the specific speech number from whence the text came. So we will create a factor that let’s us identify each speech by a number. We don’t have to do this if we want to consider all the speeches text as one big “blob”. obamaParse &lt;- function(url,write=FALSE) { library(rvest) retlist &lt;- list() df &lt;- data.frame() for (ii in 1:length(url)) { page &lt;- read_html(url[ii]) %&gt;% html_nodes(&quot;p&quot;) %&gt;% html_text() # The following gets rid of some links that don&#39;t work. See, this is # the crap you have to deal with when working with real data val &lt;- grep(&quot;Against the Iraq&quot;,page) page &lt;- page[-1:-val] # Now we also want to get rid of numbers since they generally # don&#39;t mean very much in text analysis. page_nonum &lt;- as.vector(lapply(page,function(x) gsub(&quot;[0-9]+&quot;,&quot;&quot;,x))) # We populate each list element with a one line data frame which # contains the text of one speech and its associated &quot;book&quot; number # e.g. 1,2,3... for the length of the url argument retlist[[ii]] &lt;- data.frame(text=page,book=as.character(ii), stringsAsFactors = FALSE) # Return the page } df &lt;- do.call(rbind,retlist) return(df) } Let’s get just 9 speeches to make things manageable. out &lt;- obamaParse(obamalinks[2:12]) speeches &lt;- out %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number()) %&gt;% ungroup() speeches ## # A tibble: 472 x 3 ## text book linenumber ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 &quot;Election Night Victory Speech \\n Grant P… 1 1 ## 2 &quot;&quot; 1 2 ## 3 &quot;If \\n there is anyone out there who stil… 1 3 ## 4 &quot;It’s \\n the answer told by lines that st… 1 4 ## 5 &quot;It’s \\n the answer spoken by young and o… 1 5 ## 6 &quot;It’s \\n the answer that led those who ha… 1 6 ## 7 &quot;It’s \\n been a long time coming, but ton… 1 7 ## 8 &quot;I just \\n received a very gracious call … 1 8 ## 9 &quot;I want \\n to thank my partner in this jo… 1 9 ## 10 &quot;I would \\n not be standing here tonight … 1 10 ## # … with 462 more rows 4.7 Exploring The Text So there are some general activities common to text explorations. Let’s summarize: Collections of text (one or more documents) can be called a corpus We tend to break up documents into words and treat them as a bag of words Getting the text into a tidy format is importnat. The tidytext package helps. Basically we get each word of a phrase or document onto its own line a data frame We then remove stop words which are filler words that don’t mean much We might also get rid of numbers Next, we stem the words (e.g. america and americas are really the same word) The tidytext package is what we will use here although there a number of R packages that take a more traditional approach to analyzing text such as the tm package. There is also RWeka and the qdap packages that help work with text data. You might also encounter references to things like Term Document Matrices or Document Term Matrices which are sparse matrix structures that help count the number of times a word occurs in a given set of documents. The transpose of a Document Term Matrix can be thought of as a Document Term Matrix. Here is a very brief walkthrough on how you could do some text explorations using the tm package: # https://stackoverflow.com/questions/24703920/r-tm-package-vcorpus-error-in-converting-corpus-to-data-frame#24704075 suppressMessages(library(tm)) library(&quot;wordcloud&quot;) library(&quot;RColorBrewer&quot;) # Create a vector with text x &lt;- c(&quot;Hello. Sir!&quot;,&quot;Tacos? On Tuesday?!?&quot;, &quot;Hello&quot;) # We create a Corpus mycorpus &lt;- Corpus(VectorSource(x)) # Get rid of the punctuation mycorpus &lt;- tm_map(mycorpus, removePunctuation) ## Warning in tm_map.SimpleCorpus(mycorpus, removePunctuation): transformation ## drops documents # Create a Term Document Matrix dtm &lt;- TermDocumentMatrix(mycorpus) # We can turn it onto a matrix to be read by humans m &lt;- as.matrix(dtm) v &lt;- sort(rowSums(m),decreasing=TRUE) d &lt;- data.frame(word = names(v),freq=v) head(d, 10) ## word freq ## hello hello 2 ## sir sir 1 ## tacos tacos 1 ## tuesday tuesday 1 set.seed(1234) wordcloud(words = d$word, freq = d$freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, &quot;Dark2&quot;)) https://gerardnico.com/natural_language/term_document The strengths of tm are that it more closely aligns with the traditional text mining terminology and approaches. There are also a number of tools and packages that work well with tm that do not work well directly with the tidytext approach. But there are now approaches and functions in tidytext that address this concern which can take, for example, a Term Document Matrixx and turn it into a tidy data frame. 4.7.1 Tidy Format In this class you have encountered previous references the tidyverse and tools such as those found in dplyr. It shouldn’t be surprising then that the tidytext package has text analysis tools that fit in nicely with the tidyverse. 4.7.2 Tidy Up the text Let’s look at the speeches data frame speeches ## # A tibble: 472 x 3 ## text book linenumber ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 &quot;Election Night Victory Speech \\n Grant P… 1 1 ## 2 &quot;&quot; 1 2 ## 3 &quot;If \\n there is anyone out there who stil… 1 3 ## 4 &quot;It’s \\n the answer told by lines that st… 1 4 ## 5 &quot;It’s \\n the answer spoken by young and o… 1 5 ## 6 &quot;It’s \\n the answer that led those who ha… 1 6 ## 7 &quot;It’s \\n been a long time coming, but ton… 1 7 ## 8 &quot;I just \\n received a very gracious call … 1 8 ## 9 &quot;I want \\n to thank my partner in this jo… 1 9 ## 10 &quot;I would \\n not be standing here tonight … 1 10 ## # … with 462 more rows This looks bad because the text still has junk characters in it. Each row also has a long stringy line of text (in the text column) that is not only hard to read but if we wanted to compute on the text it would be very inconvenient. It might better to first break up the long obnoxious looking line into individual words. library(tidytext) tidy_speeches &lt;- speeches %&gt;% unnest_tokens(word, text) tidy_speeches ## # A tibble: 27,849 x 3 ## book linenumber word ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 election ## 2 1 1 night ## 3 1 1 victory ## 4 1 1 speech ## 5 1 1 grant ## 6 1 1 park ## 7 1 1 illinois ## 8 1 1 november ## 9 1 1 4 ## 10 1 1 2008 ## # … with 27,839 more rows 4.7.3 Dump The “Stop Words” So we have some more work to do. Let’s get rid of common words that don’t substantially add ot the comprehensibility of the information. There is a built in data frame called stop_words that we can use to filter out such useless words (well for purposes of information retrrieval anyway). data(stop_words) tidy_speeches_nostop &lt;- tidy_speeches %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; 4.7.4 Eliminate Numbers Let’s also remove numbers from the words. # remove numbers nums &lt;- tidy_speeches_nostop %&gt;% filter(str_detect(word, &quot;^[0-9]&quot;)) %&gt;% select(word) %&gt;% unique() tidy_speeches_nostop_nonums &lt;- tidy_speeches_nostop %&gt;% anti_join(nums, by = &quot;word&quot;) tidy_speeches_nostop_nonums %&gt;% count(word, sort = TRUE) ## # A tibble: 2,736 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 change 106 ## 2 time 105 ## 3 people 99 ## 4 america 98 ## 5 american 94 ## 6 country 90 ## 7 americans 55 ## 8 care 54 ## 9 washington 54 ## 10 campaign 52 ## # … with 2,726 more rows 4.7.5 Stemming Some of these words need to be “stemmed” - such as “american” and “americans”. There is a function in the snowballC package that will allow us to do this. Here is an example. some_words &lt;- c(&quot;america&quot;,&quot;americas&quot;,&quot;american&quot;,&quot;cat&quot;,&quot;cats&quot;,&quot;catastrophe&quot;) wordStem(some_words) ## [1] &quot;america&quot; &quot;america&quot; &quot;american&quot; &quot;cat&quot; &quot;cat&quot; ## [6] &quot;catastroph&quot; tidy_speeches_nostop_nonums_stem &lt;- tidy_speeches_nostop_nonums %&gt;% mutate(word_stem = wordStem(word)) tidy_speeches_nostop_nonums_stem ## # A tibble: 9,412 x 4 ## book linenumber word word_stem ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 election elect ## 2 1 1 night night ## 3 1 1 victory victori ## 4 1 1 speech speech ## 5 1 1 grant grant ## 6 1 1 park park ## 7 1 1 illinois illinoi ## 8 1 1 november novemb ## 9 1 3 doubts doubt ## 10 1 3 america america ## # … with 9,402 more rows 4.7.6 Bar Plots library(ggplot2) tidy_speeches_nostop_nonums_stem %&gt;% count(word_stem, sort = TRUE) %&gt;% filter(n &gt; 40) %&gt;% mutate(word_stem = reorder(word_stem, n)) %&gt;% ggplot(aes(word_stem, n)) + geom_col() + xlab(NULL) + coord_flip() http://jacobsimmering.com/2016/11/15/tidytext/ library(tidyr) frequency &lt;- tidy_speeches_nostop_nonums_stem %&gt;% count(book,word_stem) %&gt;% group_by(book) %&gt;% mutate(proportion = n / sum(n)) %&gt;% select(-n) %&gt;% spread(book, proportion) %&gt;% gather(book, proportion, `1`:`9`) tidy_speeches_nostop_nonums_stem %&gt;% count(book,word_stem,sort=TRUE) %&gt;% filter(n &gt; 14) %&gt;% ggplot(aes(word_stem, n)) + geom_col() + xlab(NULL) + coord_flip() + facet_grid(book ~ .) tidy_speeches_nostop_nonums_stem %&gt;% count(book, word_stem, sort = TRUE) %&gt;% group_by(book) %&gt;% top_n(5) %&gt;% ungroup() %&gt;% ggplot(aes(reorder_within(word_stem, n, book), n,fill = book)) + geom_col(alpha = 0.8, show.legend = FALSE) + scale_x_reordered() + coord_flip() + facet_wrap(~book, scales = &quot;free&quot;) + scale_y_continuous(expand = c(0, 0)) + labs( x = NULL, y = &quot;Word count&quot;, title = &quot;Most frequent words after removing stop words&quot; ) ## Selecting by n https://richpauloo.github.io/2017-12-29-Using-tidytext-to-make-word-clouds/ 4.7.7 Wordclouds One of the premier visualizations for text documents is the word cloud. You see this a lot in newsletters and on flyers since word clouds show you frequently occurring topics in a way that makes it obvious what the most mentioned words are. pal &lt;- brewer.pal(8,&quot;Dark2&quot;) # plot the 50 most common words tidy_speeches_nostop_nonums_stem %&gt;% count(word_stem, sort = TRUE) %&gt;% with(wordcloud(word_stem, n, random.order = FALSE, max.words = 50, colors=pal)) See https://shiring.github.io/text_analysis/2017/06/28/twitter_post 4.8 Term Frequency As mentioned in the previous section, much of the content of a collectio of text contains lots of “filler” words such as “the”,“a”,“an”,“as”,“is”,“at”,“which”. Identifying these words is important in text mining and in search engine technology since you generally want to avoid them as they can interfere with a getting a good result. Stop words are also language dependent. Moreover, a given domain also has stop words. For example, when considering documents or tweets that mention kidney disease you might want to filter out specific references to “kidney disease” since you already know that you are considering kidney disease. So you might want to remove those obvious references and drill down into other words and phrases that might be important. https://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html 4.8.1 Back To The Speeches Let’s look at some examples of term frequency using collections of speeches from Obama and Romney when they were both candidates. I have two folders with 50 speeches each. library(readtext) # Makes it easy to read in raw text files # We have a folder with about 50 Obama Speeches setwd(&quot;~/Downloads/Candidates/OBAMA&quot;) filenames &lt;- list.files( pattern=&quot;*&quot;, full.names=TRUE) suppressWarnings(obama &lt;- lapply(filenames, readtext)) collected_obama &lt;- sapply(obama,function(x) x$text) length(collected_obama) ## [1] 48 # Now Let&#39;s get this into tidy text. We will also put a column # that marks the text with its respective source (&quot;Obama&quot;,&quot;Romney&quot;) obama_df &lt;- tibble(line=1:length(collected_obama), text=collected_obama, book=&quot;obama&quot;) # Let&#39;s tidy this up in accordance with tidy principles tidy_obama_df &lt;- obama_df %&gt;% unnest_tokens(word, text) tidy_obama_df ## # A tibble: 188,999 x 3 ## line book word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 obama a ## 2 1 obama couple ## 3 1 obama of ## 4 1 obama people ## 5 1 obama i ## 6 1 obama just ## 7 1 obama want ## 8 1 obama to ## 9 1 obama acknowledge ## 10 1 obama first ## # … with 188,989 more rows Next we will do the same for the Romney speeches. # We have about 50 speeches from Romney setwd(&quot;~/Downloads/Candidates/ROMNEY&quot;) filenames &lt;- list.files( pattern=&quot;*&quot;, full.names=TRUE) suppressWarnings(romney &lt;- lapply(filenames, readtext)) collected_romney &lt;- sapply(romney,function(x) x$text) length(collected_romney) ## [1] 49 # Now Let&#39;s get this into tidy text rom_df &lt;- tibble(line=1:length(collected_romney), text=collected_romney, book=&quot;romney&quot;) tidy_romney_df &lt;- rom_df %&gt;% unnest_tokens(word, text) # Let&#39;s combine the two data frames combined_tidy_speech &lt;- rbind(tidy_obama_df,tidy_romney_df) # Count the total number of speech words per candidate combined_tidy_speech %&gt;% group_by(book) %&gt;% summarize(count=n()) ## # A tibble: 2 x 2 ## book count ## &lt;chr&gt; &lt;int&gt; ## 1 obama 188999 ## 2 romney 90229 Next we’ll count the number of times each word occurs within speeches for each candidate: # Get number of times a word occurs within a speech # per candidate book_words &lt;- combined_tidy_speech %&gt;% count(book, word, sort=TRUE) # Get the total number of words in each speech collection total_words &lt;- book_words %&gt;% group_by(book) %&gt;% summarize(total = sum(n)) So now we can join these two data frames. What we get for our trouble is way to compute the frequency of a given term within each speech collection for each candidate. This provides information on what terms occur with great frequency and those that do not. We can plot these. book_words &lt;- left_join(book_words, total_words) ## Joining, by = &quot;book&quot; book_words ## # A tibble: 11,515 x 4 ## book word n total ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 obama the 7306 188999 ## 2 obama to 6579 188999 ## 3 obama and 6287 188999 ## 4 romney the 4364 90229 ## 5 obama that 4050 188999 ## 6 obama a 3776 188999 ## 7 romney and 3420 90229 ## 8 obama of 3361 188999 ## 9 obama you 3341 188999 ## 10 obama in 3115 188999 ## # … with 11,505 more rows library(ggplot2) ggplot(book_words, aes(n/total, fill = book)) + geom_histogram(show.legend = FALSE) + xlim(NA, 0.0009) + facet_wrap(~book, ncol = 2, scales = &quot;free_y&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 359 rows containing non-finite values (stat_bin). ## Warning: Removed 2 rows containing missing values (geom_bar). 4.9 Tf and Idf A central question in text mining and natural language processing is how to quantify what a document is about. One measure of how important a word may be is its term frequency (tf), how frequently a word occurs in a document We could use a list of stop words but it’s not always the most sophisticated approach to adjusting term frequency for commonly used words. Another approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used. t-idf similarity https://www.tidytextmining.com/tfidf.html The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites. We can use the bind_tf_idf function to compute tf, and idf for us. Furthermore it will bind this information onto a tidy text data frame. book_words &lt;- book_words %&gt;% bind_tf_idf(word, book, n) book_words ## # A tibble: 11,515 x 7 ## book word n total tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 obama the 7306 188999 0.0387 0 0 ## 2 obama to 6579 188999 0.0348 0 0 ## 3 obama and 6287 188999 0.0333 0 0 ## 4 romney the 4364 90229 0.0484 0 0 ## 5 obama that 4050 188999 0.0214 0 0 ## 6 obama a 3776 188999 0.0200 0 0 ## 7 romney and 3420 90229 0.0379 0 0 ## 8 obama of 3361 188999 0.0178 0 0 ## 9 obama you 3341 188999 0.0177 0 0 ## 10 obama in 3115 188999 0.0165 0 0 ## # … with 11,505 more rows Let’s look at a sorted list get the high tf_idf words book_words %&gt;% select(-total) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 11,515 x 6 ## book word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 romney в 106 0.00117 0.693 0.000814 ## 2 obama boo 200 0.00106 0.693 0.000733 ## 3 romney enterprise 57 0.000632 0.693 0.000438 ## 4 romney president&#39;s 55 0.000610 0.693 0.000423 ## 5 obama 250,000 107 0.000566 0.693 0.000392 ## 6 obama bargain 100 0.000529 0.693 0.000367 ## 7 obama michelle 98 0.000519 0.693 0.000359 ## 8 romney nations 44 0.000488 0.693 0.000338 ## 9 obama outstanding 82 0.000434 0.693 0.000301 ## 10 romney iran 38 0.000421 0.693 0.000292 ## # … with 11,505 more rows We can plot this information book_words %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% group_by(book) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(word, tf_idf, fill = book)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~book, ncol = 2, scales = &quot;free&quot;) + coord_flip() ## Selecting by tf_idf data(stop_words) book_words %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% group_by(book) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(word, tf_idf, fill = book)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~book, ncol = 2, scales = &quot;free&quot;) + coord_flip() ## Selecting by tf_idf 4.10 Sentiment Attempting to understand the feeling associated with words is a complicated business especially given that context can drastically change the meaning of words and phrases. Consider the following statements. To a human the sentiment is obvious. The third statement reflects sarcasm which is notoriously difficult for an algorithm to pick up. I had a good flight experience on Delta airlines. I had a bad flight experience on Delta airlines. Gee thanks Delta for losing my luggage. 4.10.1 Sentiment Dictionaries and Lexicons The tidytext package has a data frame called sentiments which has three lexicons that will help us figure out the emotional value of words. Each dictionary handles sentiment assessment differently. The nrc dictionary applies a single word emotional description to a text word Of course a word in a piece of text can be associated with different emotions. The possible sentiments are: unique(get_sentiments(&quot;nrc&quot;)$sentiment) ## [1] &quot;trust&quot; &quot;fear&quot; &quot;negative&quot; &quot;sadness&quot; ## [5] &quot;anger&quot; &quot;surprise&quot; &quot;positive&quot; &quot;disgust&quot; ## [9] &quot;joy&quot; &quot;anticipation&quot; Next there is the the afinn dictionary which presents a range of scores from -5 to 5 which coincides with a range of negative to positive emotion - although the rating is “stepped” (i.e. integers). get_sentiments(&quot;afinn&quot;) %&gt;% head(.) ## # A tibble: 6 x 2 ## word score ## &lt;chr&gt; &lt;int&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 get_sentiments(&quot;afinn&quot;) %&gt;% select(&#39;score&#39;) %&gt;% range(.) ## [1] -5 5 The bing lexicon offers a binary assessment of emotion. It’s either “negative” or “positive” so there is no continuum of emotion. unique(get_sentiments(&quot;bing&quot;)$sentiment) ## [1] &quot;negative&quot; &quot;positive&quot; # get_sentiments(&quot;bing&quot;) %&gt;% head() ## # A tibble: 6 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faced negative ## 2 2-faces negative ## 3 a+ positive ## 4 abnormal negative ## 5 abolish negative ## 6 abominable negative So next up there is the loughran lecxicon which is appropriate for financial and business contexts: unique(get_sentiments(&quot;loughran&quot;)$sentiment) ## [1] &quot;negative&quot; &quot;positive&quot; &quot;uncertainty&quot; &quot;litigious&quot; ## [5] &quot;constraining&quot; &quot;superfluous&quot; 4.10.2 An Example head(sentiments) ## # A tibble: 6 x 4 ## word sentiment lexicon score ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 abacus trust nrc NA ## 2 abandon fear nrc NA ## 3 abandon negative nrc NA ## 4 abandon sadness nrc NA ## 5 abandoned anger nrc NA ## 6 abandoned fear nrc NA txt &lt;- c(&quot;I had a good flight experience on Delta airlines&quot;, &quot;I had a bad flight experience on Delta airlines&quot;, &quot;Gee thanks Delta for losing my luggage&quot;) df &lt;- tibble(1:length(txt),text=txt) df &lt;- df %&gt;% unnest_tokens(word,text) # We get rid of the stop words and then see what the nrc sentiment # dictionary tells us about our data df %&gt;% anti_join(stop_words) %&gt;% left_join(get_sentiments(&quot;nrc&quot;)) ## Joining, by = &quot;word&quot; ## Joining, by = &quot;word&quot; ## # A tibble: 19 x 3 ## `1:length(txt)` word sentiment ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 flight &lt;NA&gt; ## 2 1 experience &lt;NA&gt; ## 3 1 delta &lt;NA&gt; ## 4 1 airlines &lt;NA&gt; ## 5 2 bad anger ## 6 2 bad disgust ## 7 2 bad fear ## 8 2 bad negative ## 9 2 bad sadness ## 10 2 flight &lt;NA&gt; ## 11 2 experience &lt;NA&gt; ## 12 2 delta &lt;NA&gt; ## 13 2 airlines &lt;NA&gt; ## 14 3 gee &lt;NA&gt; ## 15 3 delta &lt;NA&gt; ## 16 3 losing anger ## 17 3 losing negative ## 18 3 losing sadness ## 19 3 luggage &lt;NA&gt; df %&gt;% anti_join(stop_words) %&gt;% left_join(get_sentiments(&quot;bing&quot;)) ## Joining, by = &quot;word&quot; ## Joining, by = &quot;word&quot; ## # A tibble: 13 x 3 ## `1:length(txt)` word sentiment ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 flight &lt;NA&gt; ## 2 1 experience &lt;NA&gt; ## 3 1 delta &lt;NA&gt; ## 4 1 airlines &lt;NA&gt; ## 5 2 bad negative ## 6 2 flight &lt;NA&gt; ## 7 2 experience &lt;NA&gt; ## 8 2 delta &lt;NA&gt; ## 9 2 airlines &lt;NA&gt; ## 10 3 gee &lt;NA&gt; ## 11 3 delta &lt;NA&gt; ## 12 3 losing negative ## 13 3 luggage &lt;NA&gt; df %&gt;% anti_join(stop_words) %&gt;% left_join(get_sentiments(&quot;afinn&quot;)) ## Joining, by = &quot;word&quot; ## Joining, by = &quot;word&quot; ## # A tibble: 13 x 3 ## `1:length(txt)` word score ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 flight NA ## 2 1 experience NA ## 3 1 delta NA ## 4 1 airlines NA ## 5 2 bad -3 ## 6 2 flight NA ## 7 2 experience NA ## 8 2 delta NA ## 9 2 airlines NA ## 10 3 gee NA ## 11 3 delta NA ## 12 3 losing -3 ## 13 3 luggage NA 4.10.3 Speech Sentiment Let’s apply our new found knowledge of sentiment tools to the combined speeches of Romney and Obama. We might want to get some sense of how emotive their respective sets of speeches are. We might also want to look at the emotive range for all speches combined independently of the candidate. Of course, these are political speeches so you know up front that there will be a bias towards positive language. So here we will join the words from all speeches to the bing sentiment lexicon to rate the emotional content of the words comprising the speech. Note that many words in the speeches will not have an emotional equivalent because they don’t exist in the lexicon. This is why you should use a stop word approach or consider using a Term Frequency - Inverse frequency approach to first eliminate filler words. bing_word_counts &lt;- combined_tidy_speech %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(book, word, sentiment, sort = TRUE) %&gt;% ungroup() ## Joining, by = &quot;word&quot; So let’s look at some of the emotional content of the speeches independently of candidate (i.e. all speeches combined). bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() ## Selecting by n It’s also really easy to look at the emotions for each candidate just by using the book variable from the bing_word_counts data frame. bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment+book, scales = &quot;free_y&quot;) + labs(y = &quot;Contribution to sentiment&quot;, x = NULL) + coord_flip() ## Selecting by n So if we look at a word cloud of combined sentiment across all speeches independently of candidate we see the following: pal &lt;- brewer.pal(8,&quot;Dark2&quot;) # plot the 50 most common words bing_word_counts %&gt;% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal)) 4.10.4 Comparison Cloud So another thing we can do is to look at a comparison cloud that would give us a word cloud that presents a visualization of the most common postive and negative words. There is a function called comparison.cloud() that will accomplish this but it also requires us to do some conversion on our data frame. library(reshape2) combined_tidy_speech %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100) ## Joining, by = &quot;word&quot; We could also look at this on a per candidate basis: library(reshape2) combined_tidy_speech %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(book==&quot;romney&quot;) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100) ## Joining, by = &quot;word&quot; ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): liberty could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): benefits could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): promises could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): recovery could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): innovation could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): greatness could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): powerful could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): destiny could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): proud could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): honor could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): commitment could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): confidence could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): courage could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): easy could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): protect could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): tough could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): extraordinary could not be fit on page. It will not be plotted. ## Warning in comparison.cloud(., colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = ## 100): wonder could not be fit on page. It will not be plotted. "],
["twitter-social-media.html", "Chapter 5 Twitter - Social Media 5.1 Twitter Uses 5.2 Why Should You Care ? 5.3 Twitter Anatomy 5.4 Accessing From R - rtweet 5.5 Authenticating From R 5.6 Some Basic Tips 5.7 Looking for People Tweeting About COPD 5.8 Cleaning and Tidying 5.9 Make The Word Cloud 5.10 Bi-Grams", " Chapter 5 Twitter - Social Media 5.1 Twitter Uses Tweets come from many sources - people, politicians, media outlets, sports teams, movie stars, etc Multimedia content can be sent along with a Tweet (pics, movies, emojis) Real time communication Anyone can create an account and also sign up to use the API Proxy news service 5.2 Why Should You Care ? That’s really up to you and your personal interests but consider that tweets can be worth a lot of money. https://www.freevaluator.com/tools/twitter-username-worth https://www.tweetbinder.com/blog/economic-value-tweet/ 5.3 Twitter Anatomy User Name: A unique userid Time Stamp: When the tweet went out Text: The content of the tweet Hashtags: Words prefixed with a # character. Links: Hyper links to other web sources Replies: Replies to a posted tweet Retweets: When someone shares a third party tweet with followers Favorites: A history of tweets you have liked Latitude / Longitude: Some tweets have geo coding information 5.4 Accessing From R - rtweet The rtweet package is very cool but you have to do the following: Install the rtweet package Setup a Twitter account. It’s free at https://twitter.com/ Note that it requires your cell number to create an App (But you can delete the account after the class is over). After you have created your account then go http://apps.twitter.com From with your account create an ”application”so you can mine other tweets: Name: rtweet testing Description: Testing Account for rtweet Website: http://twitter.com/userid (replace userid with your userid) Callback URL: http://127.0.0.1:1410 Create App 5.5 Authenticating From R Now within R, You need to do the following. You only need to do it once per session to get authenticated to use Twitter from within R. # Running this code will contact Twitter and load a web page. # If authentication is successful then you will see a web page # with the contents of library(rtweet) twitter_token &lt;- create_token( app = &quot;rtweetpitt&quot;, consumer_key = &quot;HBNyMiXbPsdsdiiisdoodvIUBsBpRY8zH&quot;, consumer_secret = &quot;Tl6uTC4quNGONXsddLoRSIn9ULe7LReHPyYleUMUlM&quot;) # Create a Function to Pull in Some Tweets my_search_tweets &lt;- function(string=&quot;Katy Perry&quot;,n=500) { require(rtweet) tweet_table &lt;- search_tweets(string,n=n, retryonratelimit = TRUE, include_rts = FALSE, lang = &quot;en&quot;) return(plain_tweets(tweet_table$text)) } raw_tweets &lt;- my_search_tweets(&quot;Katy Perry&quot;) katy_raw_tweets &lt;- raw_tweets # save for later [1] &quot;i blame my bad eating habits on katy perry. idk why but i do.&quot; [2] &quot;When I was 10, I slept over at my friend&#39;s house and she made me watch the Katy Perry movie but little does she know, when she fell asleep, I changed it to a JFK conspiracy documentary&quot; But wait. How can these be worth $86,904 each ? Well these are tweets ABOUT Katy Perry. Let’s get some tweets that she herself actually issued and then see if they are worth that much. kpt &lt;- get_timeline(&quot;katyperry&quot;,n=100) &gt; plain_tweets(kpt$text)[1:9] [1] &quot;I don&#39;t know if anyone noticed but I almost broke down in tears having to pick one of my children #americanidol&quot; [2] &quot;Daydreaming of a world where we have 14 #americanidol(s) (cause this SKRESSFUL)//t.co/o3u4eYRt4d&quot; [3] &quot;Is anyone else stress snacking during the commercial breaks? #americanidol&quot; [4] &quot;From 15 years of being a coachella festival goer to guesting with @zedd on his main stage set not gonna lie feels pretty chill. Thanks area for the shine by @ronyalwin @ you&#39;re doing great//t.co/gwU12MpAla&quot; [5] &quot;we must protect wherearetheavocados, beings like her don&#39;t enter our orbit often @ Coachella, California//t.co/078jLTZQXy&quot; [6] &quot;You think your favs are safe, think again. @AmericanIdol//t.co/xF53nLvGwj&quot; [8] &quot;It&#39;s time for y&#39;all to pull out your hair over these contestants #JesusTakeTheWheel #AmericanIdol//t.co/1vxNkFTaYp&quot; [9] &quot;After you&#39;re done watching Idol, East Coast (and before yours begins, West Coast), you might want to check out the Coachella live stream... around 7:30pm poss? //t.co/tXVz4J1Ra4 - Channel 1 #GetYouAGirlWhoCanDoBoth&quot; 5.6 Some Basic Tips The rtweet package is a well written and very full featured package that does a lot for you so you don’t have to. However, you do need to become familiar with the various options to get usable text. For example maybe you don’t want retweets in the tweets or user handles. You might also want tweets from a specific location or in a specific language. # Get tweets in French nd &lt;- search_tweets(&quot;#notredame&quot;,include_rts=FALSE,lang=&quot;fr&quot;) plain_text(nd$text)[95:100] &gt; plain_tweets(nd$text)[95:99] [1] &quot;#LCI le propagandiste d&#39;extrme droite Andre Bercoff raconte n&#39;importe quoi en prtendant qu&#39;il y a eu des milliers de tweets se flicitant de l&#39;incendie de #notredame c&#39;est parfaitement faux #24hpujadas//t.co/vgU8mGutCV&quot; [2] &quot;Le Prsident de la Rpublique @EmmanuelMacron s&#39;adressera la Nation ce soir, 20h. #NotreDame #NotreDameCathedralFire #Notre_Dame_de_Paris #Macron20h#Macron//t.co/GwxoUt5lsj&quot; [3] &quot;Zineb chie.... Volume 20 opus 34.... A dguster au fond des chiottes.... #NotreDame//t.co/PUmIJtrsYl&quot; [4] &quot;\\&quot;#NotreDame a tenu. Ses votes millnaires sont restes debout. Certes, Notre-Dame est abme, mutile, mais elle continuera de vivre. Ds lors que les piliers tiennent et portent l&#39;difice, il est toujours possible de reconstruire.\\&quot; #NotreDameDeParis//t.co/6YYo3QtGhU&quot; [5] &quot;Jamais un monument n&#39;avait autant touch le cur de l&#39;humanit #NotreDame #SauverNotreDame&quot; # English &gt; nd &lt;- search_tweets(&quot;#notredame&quot;,include_rts=FALSE,lang=&quot;en&quot;) Searching for tweets... Finished collecting tweets! &gt; plain_tweets(nd$text)[95:100] [1] &quot;Gut wrenching but still hauntingly beautiful. Amazing work by the Paris fire teams to preserve so much of the glasswork. #notredame//t.co/smoZaLqXwG&quot; [2] &quot;Ancient Notre Dame artifact found preserved #NotreDame//t.co/V8lZ8lE9zW&quot; [3] &quot;@joshscampbell There you go again criticizing Mr. Trump. Clearly, more raking could have saved the roof, an inspired idea. @POTUS @realDonaldTrump @NotreDame #NotreDame @PressSec @VP&quot; [4] &quot;To all the people thinking #notredame should not be rebuilt and the money given to the poor because that&#39;s what Jesus would want. You are wrong. No where in the Bible does Jesus say don&#39;t make/do beautiful costly things for me. In fact the opposite, Jesus went to the temple....&quot; [5] &quot;I keep singing &#39;The Bells of Notre Dame&#39; from the Disney movie... I know it&#39;s inappropriate but, damn, the place and that film are so iconic. The place stands for humanity and it must be fully restored. #NotreDame&quot; [6] &quot;Lot of empathetic discussion on Twitter by Mexicans about yesterday&#39;s conflagration at #NotreDame in Paris. \\&quot;How would we feel if #ChichenItza were destroyed,\\&quot; is a common refrain. Chichen Itza WAS destroyed and left to ruin; Consolidation and/or restoration began only in 1923.//t.co/6d9EhAvXz4&quot; 5.7 Looking for People Tweeting About COPD Let’s look at something more serious: raw_tweets &lt;- my_search_tweets(&quot;COPD&quot;) raw_tweets [995] &quot;Rebecca lives with COPD and is being treated for non-tuberculous mycobacterial infection (NTM) as well. She&#39;s looking forward to being clear of the NTM infection after 18 months&#39; treatment. //t.co/Vo7L5Fdco1&quot; [996] &quot;Do you have COPD? Are you receiving the care that you&#39;re entitled to? Complete your online COPD patient passport to find out now. Discover if you&#39;re getting the care that you are eligible for, and what to talk to your doctor about if not //t.co/9Jyonn2PeG//t.co/js88XSHF0E&quot; [997] &quot;Is #COPD associated with alterations in #hearing? A systematic review meta-analysis://t.co/jnXA6boq0N//t.co/KYfOxgL4aF&quot; [998] &quot;omg this is like when im teaching COPD clients breathing exercises in hospital but now i have to use it on myself CALM DOWN MY HEART #PERSONAWelcomeParty&quot; [999] &quot;#mdpidiagnostics Predicting #Pulmonary Function Testing from Quantified Computed #Tomography Using Machine Learning Algorithms in Patients with #COPD @UniHeidelberg @UBMannheim @dzhk_germany //t.co/FppyxJ5J5F//t.co/PspkxCLdKj&quot; [1000] &quot;@KTHopkins Hidden disabilities such as COPD, MS, Crohn&#39;s disease and arthritis can affect things like pain, stamina, breathing, and other things that influence how far someone can walk. When you are old or unwell, it will be you in a wheelchair someday. Pray that people are kind, instead.&quot; So you might want to look at the data structure that is returned because it can give a lot of information for your research. copd_tweets &lt;- search_tweets(&quot;#COPD&quot;) # Here are all the variables associated with each tweet # There are 88 columns worth of information ! names(copd_tweets) [1] &quot;user_id&quot; &quot;status_id&quot; [3] &quot;created_at&quot; &quot;screen_name&quot; [5] &quot;text&quot; &quot;source&quot; [7] &quot;display_text_width&quot; &quot;reply_to_status_id&quot; [9] &quot;reply_to_user_id&quot; &quot;reply_to_screen_name&quot; [11] &quot;is_quote&quot; &quot;is_retweet&quot; [13] &quot;favorite_count&quot; &quot;retweet_count&quot; [15] &quot;hashtags&quot; &quot;symbols&quot; [17] &quot;urls_url&quot; &quot;urls_t.co&quot; [19] &quot;urls_expanded_url&quot; &quot;media_url&quot; [21] &quot;media_t.co&quot; &quot;media_expanded_url&quot; [23] &quot;media_type&quot; &quot;ext_media_url&quot; [25] &quot;ext_media_t.co&quot; &quot;ext_media_expanded_url&quot; [27] &quot;ext_media_type&quot; &quot;mentions_user_id&quot; [29] &quot;mentions_screen_name&quot; &quot;lang&quot; [31] &quot;quoted_status_id&quot; &quot;quoted_text&quot; [33] &quot;quoted_created_at&quot; &quot;quoted_source&quot; [35] &quot;quoted_favorite_count&quot; &quot;quoted_retweet_count&quot; [37] &quot;quoted_user_id&quot; &quot;quoted_screen_name&quot; [39] &quot;quoted_name&quot; &quot;quoted_followers_count&quot; [41] &quot;quoted_friends_count&quot; &quot;quoted_statuses_count&quot; [43] &quot;quoted_location&quot; &quot;quoted_description&quot; [45] &quot;quoted_verified&quot; &quot;retweet_status_id&quot; [47] &quot;retweet_text&quot; &quot;retweet_created_at&quot; [49] &quot;retweet_source&quot; &quot;retweet_favorite_count&quot; [51] &quot;retweet_retweet_count&quot; &quot;retweet_user_id&quot; [53] &quot;retweet_screen_name&quot; &quot;retweet_name&quot; [55] &quot;retweet_followers_count&quot; &quot;retweet_friends_count&quot; [57] &quot;retweet_statuses_count&quot; &quot;retweet_location&quot; [59] &quot;retweet_description&quot; &quot;retweet_verified&quot; [61] &quot;place_url&quot; &quot;place_name&quot; [63] &quot;place_full_name&quot; &quot;place_type&quot; [65] &quot;country&quot; &quot;country_code&quot; [67] &quot;geo_coords&quot; &quot;coords_coords&quot; [69] &quot;bbox_coords&quot; &quot;status_url&quot; [71] &quot;name&quot; &quot;location&quot; [73] &quot;description&quot; &quot;url&quot; [75] &quot;protected&quot; &quot;followers_count&quot; [77] &quot;friends_count&quot; &quot;listed_count&quot; [79] &quot;statuses_count&quot; &quot;favourites_count&quot; [81] &quot;account_created_at&quot; &quot;verified&quot; [83] &quot;profile_url&quot; &quot;profile_expanded_url&quot; [85] &quot;account_lang&quot; &quot;profile_banner_url&quot; [87] &quot;profile_background_url&quot; &quot;profile_image_url&quot; So you could spend a lot of time reviewing this information and you probably should if you want to mine Twitter for stuff. Let’s look at the user names of the people who tweeted about COPD. copd_tweets$screen_name [1] &quot;YouThisMe&quot; &quot;ATS_GG&quot; &quot;JayVFight&quot; [4] &quot;GKA_field&quot; &quot;ForaCareUSA&quot; &quot;littlelisa3579&quot; [7] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;COPD_research&quot; [10] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;COPD_research&quot; [13] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;COPD_research&quot; [16] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;COPD_research&quot; [19] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;COPD_research&quot; [22] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;COPD_research&quot; [25] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;COPD_research&quot; [28] &quot;COPD_research&quot; &quot;COPD_research&quot; &quot;rch4him&quot; [31] &quot;ACMeinde&quot; &quot;efrasanchez&quot; &quot;efrasanchez&quot; [34] &quot;efrasanchez&quot; &quot;myCOPDteam&quot; &quot;ATS_BSHSR&quot; [37] &quot;AllergyAsthmaHQ&quot; &quot;guillesole3&quot; &quot;hgosker&quot; [40] &quot;MaryGreerMurder&quot; &quot;SmartVestSystem&quot; &quot;patosidro&quot; [43] &quot;Sandip_Thakrar&quot; &quot;TheCQRC&quot; &quot;CaliDiet&quot; [46] &quot;GetPalliative&quot; &quot;HildaRosaCarta1&quot; &quot;IARETUSA&quot; [49] &quot;DrMajzun&quot; &quot;ChrisCarrollMD&quot; &quot;accpchest&quot; [52] &quot;CResnews&quot; &quot;MorphCLtd&quot; &quot;COPDCanada&quot; [55] &quot;DialysisSaves&quot; &quot;FABIOVARON&quot; &quot;Donicme&quot; [58] &quot;DemFromCT&quot; &quot;mariaalerey&quot; &quot;ChronicRights&quot; [61] &quot;virenkaul&quot; &quot;atscommunity&quot; &quot;eczemasupport&quot; [64] &quot;BENESG&quot; &quot;DAMiD_Presse&quot; &quot;datadrivencare&quot; [67] &quot;caring_mobile&quot; &quot;RFalfanV&quot; &quot;r2guidance&quot; [70] &quot;EverFLO_Q_OPI&quot; &quot;EverFLO_Q_OPI&quot; &quot;CCI_Ltd&quot; [73] &quot;Sergioblasco68&quot; &quot;trinity_delta&quot; &quot;Nurse_JSW&quot; [76] &quot;LiddleCarol&quot; &quot;pulmonology101&quot; &quot;cristineeberry&quot; [79] &quot;Ms_MMM_Herbert&quot; &quot;AnnieBruton&quot; &quot;jt_resp_physio&quot; [82] &quot;_RichardPalmer&quot; &quot;KarenFinnLondon&quot; &quot;russwinn66&quot; [85] &quot;Ilio79&quot; &quot;teraokanozomi&quot; &quot;AnnaFla1268&quot; [88] &quot;PCRSUK&quot; &quot;lunguk&quot; &quot;bullringbash1&quot; [91] &quot;exerciseCOPD&quot; &quot;blfwales&quot; &quot;666Dunst&quot; [94] &quot;Atemwegsliga&quot; &quot;D_mk77&quot; &quot;AJ_MCMLXV&quot; [97] &quot;Health_Editor&quot; &quot;PR_Assembly&quot; &quot;AnaHDeakinQPS&quot; [100] &quot;RespirologyAPSR&quot; 5.8 Cleaning and Tidying Tweets are much “dirtier” than the text we saw associated with the candidate speeches. To make a reasonable attempt at analyzing them in any way we will need to eliminate URLs and special characters. Consider this tweet. It’s really messy. The rtweet package has a function called plain_tweets that allows one to strip out some of the junk [98] &quot;Work from our own @suryapbhatt and fearless leader @realmdransfield on video #telehealth #cardiopulmonary #rehabilitation and reduction in #COPD readmissions. \\n\\n#gamechanger #UABeMedicine \\n\\nhttps://t.co/6XxIT14bSs\\n\\n@pulmonary_rehab @MTMaddocks @atscommunity @uabmedicine \\n@RKalhan&quot; # plain_tweets(rawt) -&gt; out [1] &quot;Work from our own @suryapbhatt and fearless leader @realmdransfield on video #telehealth #cardiopulmonary #rehabilitation and reduction in #COPD readmissions. #gamechanger #UABeMedicine //t.co/6XxIT14bSs @pulmonary_rehab @MTMaddocks @atscommunity @uabmedicine @RKalhan&quot; Let’s eliminate a lot of junk library(stringr) out &lt;- &quot;Work from our own @suryapbhatt and fearless leader @realmdransfield on video #telehealth #cardiopulmonary #rehabilitation and reduction in #COPD readmissions. #gamechanger #UABeMedicine //t.co/6XxIT14bSs @pulmonary_rehab @MTMaddocks @atscommunity @uabmedicine @RKalhan&quot; #get rid of unnecessary spaces clean_tweet &lt;- str_replace_all(out,&quot; &quot;,&quot; &quot;) # Get rid of URLs clean_tweet &lt;- str_replace_all(clean_tweet, &quot;https://t.co/[a-z,A-Z,0-9]*&quot;,&quot;&quot;) clean_tweet &lt;- str_replace_all(clean_tweet, &quot;http://t.co/[a-z,A-Z,0-9]*&quot;,&quot;&quot;) clean_tweet &lt;- str_replace_all(clean_tweet, &quot;//t.co/[a-z,A-Z,0-9]*&quot;,&quot;&quot;) # Take out retweet header, there is only one clean_tweet &lt;- str_replace(clean_tweet,&quot;RT @[a-z,A-Z]*: &quot;,&quot;&quot;) # Get rid of hashtags clean_tweet &lt;- str_replace_all(clean_tweet,&quot;#[a-z,A-Z]*&quot;,&quot;&quot;) # Get rid of references to other screennames clean_tweet &lt;- str_replace_all(clean_tweet,&quot;@[a-z,A-Z]*&quot;,&quot;&quot;) # Get rid of newlines clean_tweet &lt;- str_replace_all(clean_tweet,&quot;\\n&quot;,&quot;&quot;) # Get rid of underscores clean_tweet &lt;- str_replace_all(clean_tweet,&quot;_&quot;,&quot;&quot;) Now let’s look at the output. Much better clean_tweet [1] &quot;Work from our own and fearless leader on video and reduction in readmissions. rehab &quot; We could then apply this our raw COPD tweets library(stringr) out &lt;- raw_tweets #get rid of unnecessary spaces clean_tweet &lt;- str_replace_all(out,&quot; &quot;,&quot; &quot;) # Get rid of URLs clean_tweet &lt;- str_replace_all(clean_tweet, &quot;https://t.co/[a-z,A-Z,0-9]*&quot;,&quot;&quot;) clean_tweet &lt;- str_replace_all(clean_tweet, &quot;http://t.co/[a-z,A-Z,0-9]*&quot;,&quot;&quot;) clean_tweet &lt;- str_replace_all(clean_tweet, &quot;//t.co/[a-z,A-Z,0-9]*&quot;,&quot;&quot;) # Take out retweet header, there is only one clean_tweet &lt;- str_replace(clean_tweet,&quot;RT @[a-z,A-Z]*: &quot;,&quot;&quot;) # Get rid of hashtags clean_tweet &lt;- str_replace_all(clean_tweet,&quot;#[a-z,A-Z]*&quot;,&quot;&quot;) # Get rid of references to other screennames clean_tweet &lt;- str_replace_all(clean_tweet,&quot;@[a-z,A-Z]*&quot;,&quot;&quot;) # Get rid of newlines clean_tweet &lt;- str_replace_all(clean_tweet,&quot;\\n&quot;,&quot;&quot;) # Get rid of underscores clean_tweet &lt;- str_replace_all(clean_tweet,&quot;_&quot;,&quot;&quot;) Next we’ll create a data frame and then tokenize the COPD tweets copd_df &lt;- tibble(line=1:length(clean_tweet),text=clean_tweet) # Next we&#39;ll tokenize them tidy_copd_df &lt;- copd_df %&gt;% unnest_tokens(word,text) tidy_copd_df &lt;- tidy_copd_df %&gt;% anti_join(stop_words) tidy_copd_df # A tibble: 26,103 x 2 line word &lt;int&gt; &lt;chr&gt; 1 1 6 2 1 haha 3 1 final 4 1 understand 5 1 copd 6 1 asthma 7 1 neurologic 8 1 psychiatric 9 1 disorders 10 1 addiction 5.9 Make The Word Cloud tidy_copd_df_table &lt;- tidy_copd_df %&gt;% count(word, sort = TRUE) tidy_copd_df_table # https://www.littlemissdata.com/blog/wordclouds wordcloud2(tidy_copd_df_table,size=0.7) Let’s try that again because the word copd dominates the cloud. swords &lt;- c(&quot;copd&quot;,&quot;patients&quot;,&quot;disease&quot;,&quot;health&quot;,&quot;chronic&quot;, &quot;pulmonary&quot;,&quot;obstructive&quot;,&quot;lung&quot;,&quot;1&quot;,&quot;2&quot;) tidy_copd_df_table &lt;- tidy_copd_df_table %&gt;% filter(!word %in% swords) wordcloud2(tidy_copd_df_table,size=0.7) 5.10 Bi-Grams copd_df &lt;- tibble(line=1:length(clean_tweet),text=clean_tweet) # Next we&#39;ll tokenize them bi_tidy_copd_df &lt;- copd_df %&gt;% unnest_tokens(paired_words,text,token = &quot;ngrams&quot;, n = 2) bi_tidy_copd_df %&gt;% count(paired_words, sort = TRUE) # bi_tidy_copd_df_separated_words &lt;- bi_tidy_copd_df %&gt;% separate(paired_words, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) swords &lt;- c(&quot;copd&quot;,&quot;patients&quot;,&quot;disease&quot;,&quot;health&quot;,&quot;chronic&quot;, &quot;pulmonary&quot;,&quot;obstructive&quot;,&quot;lung&quot;,&quot;1&quot;,&quot;2&quot;) bi_copd_tweets_filtered &lt;- bi_tidy_copd_df_separated_words %&gt;% filter(!word1 %in% c(stop_words$word,swords)) %&gt;% filter(!word2 %in% c(stop_words$word,swords)) &gt; bi_copd_tweets_filtered # A tibble: 9,078 x 3 line word1 word2 &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 6 haha 2 1 psychiatric disorders 3 1 disorders addiction 4 1 addiction pain 5 1 infectious diseases 6 3 run speed 7 3 speed walk 8 3 walk cycle 9 3 cycle workout 10 3 canada army # … with 9,068 more rows copd_bi_words_counts &lt;- bi_copd_tweets_filtered %&gt;% count(word1,word2,sort=TRUE) &gt; copd_bi_words_counts # A tibble: 7,388 x 3 word1 word2 n &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 wearables trial 24 2 eu patent 20 3 heart failure 19 4 phase 3 15 5 3 data 14 6 immunization program 14 7 maternal immunization 14 8 program phase 14 9 quit smoking 14 10 rsv maternal 14 # … with 7,378 more rows # library(igraph) library(ggraph) copd_bi_words_counts %&gt;% filter(n &gt;= 10) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 3) + geom_node_text(aes(label = name), vjust = 1.8, size = 3) + labs(title = &quot;Word Network: Tweets using the hashtag - COPD&quot;, subtitle = &quot;Text mining twitter data &quot;, x = &quot;&quot;, y = &quot;&quot;) "]
]
